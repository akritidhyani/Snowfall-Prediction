# -*- coding: utf-8 -*-
"""Machine_Learning_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ovft9c9fqfeYl912qTCwaAQbc40L6hTx
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install xarray

import xarray as xr
import os

!pip install netCDF4

from netCDF4 import Dataset

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/raw_data_netcdf_format.nc'
data = Dataset(file_path)

!pip install netcdf2csv

from netcdf2csv import convert_file

convert_file(file_path,'/content/drive/MyDrive','/content/drive/MyDrive',clean_choice=0)

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/raw_data_csv_format.csv")
print(df)

print(f"This dataset has a total of {len(df.columns)} features and they are as follow {list(df.columns)}")

df.rename(columns = {'u100':'100m u componenet of wind', 'v100':'100m v-component of wind',
                              'u10n':'10m u-componenet of wind','v10n':'10m v-componenet of wind','fg10':'10m wind gust','d2m':'2m dewpoint temperature','t2m':'2m temperature','anor':'Angle of sub-gridscale orography','isor':'Anisotropy of sub-gridscale orography'}, inplace = True)

df.rename(columns = {'bld':'Boundary layer dissipation', 'blh':'Boundary layer height','cbh':'Cloud base height','cin':'Convective inhibition','csf':'Convective snowfall','csfr':'Convective snowfall rate water equivalent','lgws':' Eastward gravity wave surface stress','ewss':'Eastward turbulent surface stress','zust':'Friction velocity','z':'Geopotential','gwd':'Gravity wave dissipation','hcc':'High cloud cover','lsf':'Large scale snowfall rate water equivalent','lssfr':'Large-scale snowfall','lcc':'Low cloud cover','msl':'Mean sea level pressure'}, inplace = True)

df.rename(columns = {'mser':'Mean snow evaporation rate', 'msr':'Mean snowfall rate','cbh':'Cloud base height','mcc':'Medium cloud cover','ptype':'Precipitation type','sp':'Surface pressure','tcsw':' Total column snow water','tcslw':'Total column supercooled liquid water','deg0l':'Zero degree level'}, inplace = True)

df.head()

df['time'] = pd.to_datetime(df['time'])
df.head()

df['time']=df['time'].dt.time

df.head()

# import libraries
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# set figure size
plt.figure(figsize=(36,36))

# Generate a mask to onlyshow the bottom triangle
mask = np.triu(np.ones_like(df.corr(), dtype=bool))

# generate heatmap
sns.heatmap(df.corr(), annot=True, mask=mask, vmin=-1, vmax=1)
plt.title('Correlation Coefficient Of Predictors')
plt.show()

# load statmodels functions
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# compute the vif for all given features
def compute_vif(considered_features):

    X = df[considered_features]
    # the calculation of variance inflation requires a constant
    X['intercept'] = 1

    # create dataframe to store vif values
    vif = pd.DataFrame()
    vif["Variable"] = X.columns
    vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    vif = vif[vif['Variable']!='intercept']
    return vif
# features to consider removing
considered_features = ['100m u componenet of wind','100m v-component of wind','10m u-componenet of wind','10m v-componenet of wind','2m dewpoint temperature','2m temperature','Surface pressure','Zero degree level','Geopotential','latitude','Boundary layer dissipation',' Eastward gravity wave surface stress','Eastward turbulent surface stress','Gravity wave dissipation','Large scale snowfall rate water equivalent','Large-scale snowfall']


# compute vif
compute_vif(considered_features).sort_values('VIF', ascending=False)

# compute vif values after removing a feature
considered_features.remove('Surface pressure')
compute_vif(considered_features).sort_values('VIF', ascending=False)

"""LOGISTIC REGRESSION"""

# compute vif values after removing a feature
considered_features.remove('2m dewpoint temperature')
compute_vif(considered_features).sort_values('VIF', ascending=False)

# compute vif values after removing a feature
considered_features.remove('Large scale snowfall rate water equivalent')
compute_vif(considered_features).sort_values('VIF', ascending=False)

# compute vif values after removing a feature
considered_features.remove('Eastward turbulent surface stress')
compute_vif(considered_features).sort_values('VIF', ascending=False)

# compute vif values after removing a feature
considered_features.remove('Zero degree level')
compute_vif(considered_features).sort_values('VIF', ascending=False)

# compute vif values after removing a feature
considered_features.remove('Gravity wave dissipation')
compute_vif(considered_features).sort_values('VIF', ascending=False)

feature_cleaned_df=df.drop(['Surface pressure','2m dewpoint temperature','Large scale snowfall rate water equivalent','Eastward turbulent surface stress','Zero degree level','Gravity wave dissipation'],axis=1)
feature_cleaned_df.head()

feature_cleaned_df=feature_cleaned_df.dropna()
feature_cleaned_df

list(feature_cleaned_df.columns)

"""## Linear Regression"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

scaler = MinMaxScaler()
features = [ 'longitude',
 'latitude',
 '100m u componenet of wind',
 '100m v-component of wind',
 '10m u-componenet of wind',
 '10m v-componenet of wind',
 '10m wind gust',
 '2m temperature',
 'Angle of sub-gridscale orography',
 'Anisotropy of sub-gridscale orography',
 'Boundary layer dissipation',
 'Boundary layer height',
 'Cloud base height',
 'Convective inhibition',
 'Convective snowfall',
 'Convective snowfall rate water equivalent',
 ' Eastward gravity wave surface stress',
 'Friction velocity',
 'Geopotential',
 'High cloud cover',
 'Large-scale snowfall',
 'Low cloud cover',
 'Mean sea level pressure',
 'Mean snow evaporation rate',
 'Mean snowfall rate',
 'Medium cloud cover',
 'Precipitation type',
 ' Total column snow water',
 'Total column supercooled liquid water']
X = feature_cleaned_df[features]
y = feature_cleaned_df['Mean snowfall rate']  # Target variable (Mean Snowfall Rate)
# Handle missing values and normalize data
X = X.dropna()  # Drop rows with missing values
X_scaled = scaler.fit_transform(X)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)
print("The predicted Y values are:",y_pred)


# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

feature_cleaned_df.max()

feature_cleaned_df.mean()

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix

features = [ 'longitude',
 'latitude',
 '100m u componenet of wind',
 '100m v-component of wind',
 '10m u-componenet of wind',
 '10m v-componenet of wind',
 '10m wind gust',
 '2m temperature',
 'Angle of sub-gridscale orography',
 'Anisotropy of sub-gridscale orography',
 'Boundary layer dissipation',
 'Boundary layer height',
 'Cloud base height',
 'Convective inhibition',
 'Convective snowfall',
 'Convective snowfall rate water equivalent',
 ' Eastward gravity wave surface stress',
 'Friction velocity',
 'Geopotential',
 'High cloud cover',
 'Large-scale snowfall',
 'Low cloud cover',
 'Mean sea level pressure',
 'Mean snow evaporation rate',
 'Mean snowfall rate',
 'Medium cloud cover',
 'Precipitation type',
 ' Total column snow water',
 'Total column supercooled liquid water']
X = feature_cleaned_df[features]
threshold = 0.000010  # You can adjust this threshold based on your data
feature_cleaned_df['snow'] = (feature_cleaned_df['Mean snowfall rate'] > threshold).astype(int)
y = feature_cleaned_df['snow']
X = X.dropna()
scaler = MinMaxScaler()
y = y[X.index]    # Drop rows with missing values
X_scaled = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Initialize and train the logistic regression model
model = LogisticRegression(max_iter=1000) # Increase max_iter
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)
print("The predicted Y values are:", y_pred, end='')

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')

"""## RIDGE REGRESSION MODEL"""

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error

ridger = Ridge(alpha=1)
ridger.fit(X_train, y_train)

# Make predictions on the training set
y_predicted_train = ridger.predict(X_train)

# Evaluate the model on the training set
train_rmse = np.sqrt(mean_squared_error(y_train, y_predicted_train))
train_mae = mean_absolute_error(y_train, y_predicted_train)
print("Training RMSE:", train_rmse)
print("Training MAE:", train_mae)

# Make predictions on the test set
y_predicted_test = ridger.predict(X_test)
print(y_predicted_test)

# Evaluate the model on the test set
test_rmse = np.sqrt(mean_squared_error(y_test, y_predicted_test))
test_mae = mean_absolute_error(y_test, y_predicted_test)
print("Test RMSE:", test_rmse)
print("Test MAE:", test_mae)

"""## GRADIENT BOOSTING REGRESSOR"""

from sklearn.ensemble import GradientBoostingRegressor

features = [ 'longitude',
 'latitude',
 '100m u componenet of wind',
 '100m v-component of wind',
 '10m u-componenet of wind',
 '10m v-componenet of wind',
 '10m wind gust',
 '2m temperature',
 'Angle of sub-gridscale orography',
 'Anisotropy of sub-gridscale orography',
 'Boundary layer dissipation',
 'Boundary layer height',
 'Cloud base height',
 'Convective inhibition',
 'Convective snowfall',
 'Convective snowfall rate water equivalent',
 ' Eastward gravity wave surface stress',
 'Friction velocity',
 'Geopotential',
 'High cloud cover',
 'Large-scale snowfall',
 'Low cloud cover',
 'Mean sea level pressure',
 'Mean snow evaporation rate',
 'Mean snowfall rate',
 'Medium cloud cover',
 'Precipitation type',
 ' Total column snow water',
 'Total column supercooled liquid water']
X = feature_cleaned_df[features]
y = feature_cleaned_df['Mean snowfall rate']  # Target variable (Mean Snowfall Rate)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# Instantiate and train the model
model_gb = GradientBoostingRegressor(random_state=0)
model_gb.fit(X_train, y_train)
y_predicted_train=model_gb.predict(X_train)
train_rmse = np.sqrt(mean_squared_error(y_train, y_predicted_train))
train_mae = mean_absolute_error(y_train, y_predicted_train)
print("Training RMSE:", train_rmse)
print("Training MAE:", train_mae)


# Make predictions
y_pred_gb = model_gb.predict(X_test)
print(y_pred_gb)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))
test_mae = mean_absolute_error(y_test, y_pred_gb)
print("Test RMSE:", test_rmse)
print("Test MAE:", test_mae)

"""## Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

regr = RandomForestRegressor(max_depth=2, random_state=42)
regr.fit(X_train, y_train)
y_predicted_train_rf=regr.predict(X_train)
train_rmse = np.sqrt(mean_squared_error(y_train, y_predicted_train_rf))
train_mae = mean_absolute_error(y_train, y_predicted_train_rf)
print("Training RMSE:", train_rmse)
print("Training MAE:", train_mae)

# Make predictions
y_pred_rf = regr.predict(X_test)
print(y_pred_rf)
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
test_mae = mean_absolute_error(y_test, y_pred_rf)
print("Test RMSE:", test_rmse)
print("Test MAE:", test_mae)